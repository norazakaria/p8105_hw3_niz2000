---
title: "p8105_hw3_niz2000"
author: "Nora Zakaria"
date: "10/14/2019"
output: github_document
---

The first step of the assignment is to load tidyverse into the markdown document. 
```{r}
library(tidyverse)
```

# Problem 1

Problem 1 uses Instacart Online Grocery Shopping Dataset from 2017, loaded from the course datasets, and not in my local data directory for the p8105_hw3_niz20000 project.

## Load the Instacart Dataset
The first step for problem 1 is to load the instacart dataset. 
```{r}
library(p8105.datasets)
data("instacart")
instacart
```
The Instacart dataset was loaded, and consists of 1,384,617 observations and 15 variables from 131,209 unique users, where each row in the dataset is a product from an order. Key variables to interpret observations in the data and that are relevant to future analyses include the "order_id," "product_id," "reordered," "days_since_prior_order," "product_name," "aisle," and the "department" variables. Using the dataset and these key variables, information about specific ordering behavior for an individual can be determined. For example, order identifier 1 (order_id) ordered product number 49302 (product_id=49302), or Bulgarian Yogurt (product_name), from the dairy eggs department (department), and reordered the product an additional time (reordered). Order identifier 36 (order_id) ordered water seltzer sparkling water (product_name) from the beverages department (department), 7 days after their last instacart order was placed (days_since_prior_order).

## Aisles and Where the Most Items are Ordered From
This next step answers the question how many aisles there are in the instacart data, as well as which aisles are the most items ordered from. This involved finding the total number of items ordered per aisle, and ordering the aisles by descending, where the aisle with the most items ordered is at the top and the aisle with the least items ordered is at the bottom. 
```{r}
aisles = instacart %>%
  count(aisle)  %>%
  arrange(desc(n)) 
aisles
```
There are `r nrow(aisle)` aisles in the instacart dataset. The most items ordered were from fresh vegetables aisle at 150,609 items followed by the fresh fruits aisles at 150,473 items. The aisle with the next highest items ordered was the packaged vegetables fruits aisle, at 78,493 items.

## Plot of the Number of Items Ordered Per Aisle
In this next step, I created a plot to demonstrate the number of items ordered in each aisle, limited to aisles that had more than 10,000 items ordered. To do so, I filtered the "n" variable representing the total number of items ordered per aisle to only include aisles that had more than 10000 items ordered. To arrange aisles sensibly in the plot, I reordered the aisles so that the plot would display aisles from greatest to least items ordered, from left to right. To increase readability, I formatted the aisle names vertically under each bar in the bar graph, and added labels for the graph and axes.
```{r}
Aisle_items_plot = instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  arrange(desc(n)) %>%
  rename(n_items_ordered = n) %>%
  ggplot(aes(x = reorder(aisle, -n_items_ordered), y = n_items_ordered)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(
    title = "Items Ordered per Aisle",
    x = "Aisle Name",
    y = "Number of Items Ordered")
Aisle_items_plot
```

## Table Demonstrating Popular Items in 3 Different Aisles
This next step creates a table displaying the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits,” and the number of times that each item was ordered. The three aisle names are displayed in the leftmost column, and the products and the number of times they were ordered are displayed in the middle and rightmost columns, ordered by the 3rd most popular item at the top to the most popular item at the bottom, within each of the aisles.
```{r}
instacart %>%
  group_by(aisle, product_name) %>%
  summarize(
    n_items_ordered = n()
    ) %>%
  group_by(aisle) %>%
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"),
    min_rank(desc(n_items_ordered)) <4 ) %>%
  arrange(n_items_ordered, aisle) %>%
  knitr::kable(format = 'pandoc' , caption = "Table: Most Popular Items by Aisle")
```
The three most popular products in the dog food care aisle were small dog biscuits, organix chicken and brown rice recipe, and snack sticks chicken and rice recipe dog treats, ordered 26, 28, and 30 times respectively. The three most popular products in the baking ingredients aisle were cane sugar, pure baking soda, and light brown sugar, ordered 336, 387, and 499 times respectively. The three most popular products in the packaged vegetables fruits aisle were organic blueberries, organic raspberries, and organic baby spinach, ordered 4,966, 5,546, and 9,784 times respectively. 

## Pink Lady Apples and Coffee Ice Cream
This step displays a table of the mean hour of the day at which the Pink Lady Apples and Coffee Ice Cream products are ordered on each day of the week. The final table displays the two products in the leftmost column, seven columns for the seven days of the week, and the mean hour of the day at which each product is ordered under the corresponding days of the week.
```{r}
instacart %>%
  select(product_name, order_dow, order_hour_of_day) %>% 
  filter(
    product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>%
  mutate(
    day_of_week = recode(order_dow,
      `0` = "Sunday",
      `1` = "Monday",
      `2` = "Tuesday",
      `3` = "Wednesday",
      `4` = "Thursday",
      `5` = "Friday",
      `6` = "Saturday")) %>%
  select(product_name, day_of_week, mean_hour) %>% 
  pivot_wider(
    names_from = "day_of_week",
    values_from = "mean_hour") %>% 
  knitr::kable(format= 'pandoc', caption = "Table: Mean Hour Pink Lady Apples and Coffee Ice Cream are Ordered")
```


# Problem 2

Problem 2 uses BRFSS data, loaded from the course datasets, and can not be found in my local data directory for the p8105_hw3_niz20000 project. BRFSS is a continuous, state-based surveillance system that collects information about modifiable risk factors for chronic diseases and other leading causes of death.

## Load BRFSS Dataset
The first step for problem 2 is to load the BRFSS dataset. 
```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

## Clean the BRFSS Dataset
In order to perform analyses, the BRFSS data needs to be cleaned. The data was formatted using appropriate variable names, such as renaming the locationabbr and locationdesc variables to more clearly designate which variable corresponds to a US state, and which to a county within that state, using the anmes "location_state" and "location_county." In order to focus on the Overall Health topic, the topic variable was filtered. The response variable was converted to a factor variable, with levels ordered from "Poor" to "Excellent," and any responses that did not fall within those bounds were dropped. 
```{r}
brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(-data_value_footnote_symbol, -data_value_footnote, -location_id) %>%
  drop_na(response) %>%
  mutate(response = as.factor(response),
         response = factor(response,
                           levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>%
  rename(
    location_state= locationabbr,
    location_county= locationdesc)
brfss
```


## States Observed at 7 or More Locations
In this step, the brfss data was used to determine with which states were observed at at least 7 county locations in the years 2002 and 2010. The first section of code corresponds to 2002 while the second code corresponds to 2010, through filtering the year variable accordingly. 
```{r}
# Restricting to 2002
location_2002 = brfss %>%
  filter(year == "2002") %>%
  distinct(location_state, location_county) %>%
  count(location_state) %>%
  filter(n >= 7) %>%
  rename(number_sites = n)
location_2002 

# Restricting to 2010
location_2010 = brfss %>%
  filter(year == "2010") %>%
  distinct(location_state, location_county) %>%
  count(location_state) %>%
  filter(n >= 7) %>%
  rename(number_sites = n)
location_2010
```
There were `r nrow(location_2002)` states that were observed at at least 7 county locations in 2002, and `r nrow(location_2010)` states that were observed at at least 7 county locations in 2010. 

## Spagheti Plot of Average Data Value for Excellent Responses Across Locations by State 
In this step, a new dataset called "excellent_responses" limited to only "Excellent" responses was constructed, containing, year, state, and a new variable that averaged the data_value across locations within a state, called "mean_data_value." After contructing the dataset, a “spaghetti” plot of mean_data_value over the years of the study was generated, showing a line for each state.
```{r}
excellent_responses = brfss %>%
  filter(response == "Excellent") %>%
  group_by(year, location_state) %>%
  summarise(
    mean_data_value = mean(data_value)) %>%
  ggplot(aes(x = year, y = mean_data_value, group = location_state, color = location_state)) +
    geom_line()+
    labs(title = "Graph: Average Data Value for Excellent Responses Over Time Within a State")
excellent_responses
```

## Two-Panel Plot of Data Value for Responses in NY 
In this step a two-panel plot is displayed for the years 2006 and 2010. In each panel, the distribution of the variable data_value each level of response (“Poor” to “Excellent”) is displayed as a boxplot, restricting the location_state of both panels to NY. 
```{r}
responses_2006_2010 = brfss %>%
  filter(location_state == "NY",
        year %in% c("2006", "2010")) %>%
  select(year, response, data_value) %>%
  ggplot(aes(x = response, y = data_value)) +
    geom_boxplot() +
    facet_grid(~year) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    labs(title = "Distribution of Data Values for Responses in NY State")
responses_2006_2010
```


# Problem 3

Problem 3 uses accelerometer data, loaded from the accel_data CSV file found in my local data directory "Data" in the p8105_hw3_niz20000 project. Accelerometers are devices that measure “activity counts” in a short period; in this case one-minute intervals. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations.

## Load and Tidy the Accelerometer Data 
The first step for problem 3 is to load the accel_data dataset. In order to load, tidy, and wrangle the dataset, all originally observed variables and values were preserved. However to ensure useful variable names, the day_id variable corresponding to the 35 days of observation was renamed to "day_number," and the day variable corresponding to the name of the day of the week was renamed to "day_of_the_week." An additional variable called "day_type" was created to demonstrate whether or not the day_of_the_week corresponds with a weekend or weekday. Another consideration in data cleaning to that the data has reasonable variable classes. The day of the week and day type variables are character, with the remaining week, day number, and activity variables "double" variables, which are responable as R automatically converts between dbl and integer variables. 
```{r}
accelerometer = 
  read_csv(file = "./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(
    day_type = if_else(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "Weekday", day),
    day_type = if_else(day %in% c("Saturday", "Sunday"), "Weekend", day_type)) %>%
  rename(
    day_number = day_id,
    day_of_the_week= day) %>%
  select(week, day_number, day_of_the_week, day_type, activity_1:activity_1440)
accelerometer
```
The accelerometer dataset displays five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). There are `r nrow(accelerometer)` observations in the dataset, with each corresponding to one of the 35 days of data collection on the the male participant. For each of the 35 days of observations, the week number, day number, day of the week, new day type variable (weekend vs weekday), and activity level at each minute of the day are displayed. As there are 1440 minutes in a 24 hour day, there are 1440 activity variables. The dataset has `r ncol(accelerometer)` columns, or variables with 1443 from the original dataset, and the additional 1444th variable corresponding to the newly created day_type variable. 

## Create a Table of Total Activity Per Day
In this step, I used the tidied accelerometer dataset, and aggregated activity accross each the 1440 minutes of each of the 35 days to create a new total_activity variable. I created a table demonstrating these day_number and these totals. I also included the day_of_the_week and the new variable day_type in the table, hypothesizing that perhaps there is a trend in activity level by day of the week or and whether or not there is a trend in activity on the weekend compared to on weekdays. 
```{r}
total_activity = accelerometer %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "total_activity",
    values_to = "activity") %>%
  group_by(day_number, day_of_the_week, day_type) %>%
  summarise(total_activity = sum(activity)) %>%
knitr::kable(format= 'pandoc', title = "Table: Total Activity per Day")
total_activity
```
Describe trends or patterns!!
  
## Plot of 24-Hour Activity for Each Observed Day
In this last step in problem 3, I made a plot to use accelerometer data to inspect activity over the course of each day of observation in the study. This single-panel plot displays the 24-hour activity time courses for each of the 35 days, and uses a different color to indicate which day of the week that observation took place.
```{r}
activity_plot = accelerometer %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "activity") %>%
  ggplot(aes(x = activity_number, y = activity, group = day_number, color = day_of_the_week)) +
    geom_line() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    labs(title = "Activity Throughout The Day")
activity_plot
```
Describe in words any patterns or conclusions you can make based on this graph!!
